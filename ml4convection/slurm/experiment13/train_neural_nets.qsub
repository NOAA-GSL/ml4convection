#!/bin/tcsh

#SBATCH --job-name="train_neural_nets"
#SBATCH --partition="fge"
#SBATCH --account="rda-ghpcs"
#SBATCH --qos="batch"
#SBATCH --nodes=1
#SBATCH --ntasks=8           # 8 tasks per node
#SBATCH --cpus-per-task=2
#SBATCH --ntasks-per-node=8  # 8 GPUs per node
#SBATCH --exclusive
#SBATCH --time=30:00:00
#SBATCH --array=1-81
#SBATCH --exclude=h28n12
##SBATCH --exclude=h31n03,h28n12
##SBATCH --exclude=h29n01,h26n05,h28n06,h29n07,h30n03,h30n15
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=ryan.lagerquist@noaa.gov
#SBATCH --output=train_neural_nets_%A_%a.out

module load cuda/10.1
source /scratch2/BMC/gsd-hpcs/Jebb.Q.Stewart/conda3.7/etc/profile.d/conda.csh
conda activate base

set CODE_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4convection_standalone/ml4convection"
set TEMPLATE_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4convection_models/experiment13/templates"
set TOP_OUTPUT_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4convection_models/experiment13"

set TRAINING_PREDICTOR_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4convection_project/predictors/quality_controlled/partial_grids"
set VALIDN_PREDICTOR_DIR_NAME="${TRAINING_PREDICTOR_DIR_NAME}"
set TRAINING_TARGET_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4convection_project/targets/new_echo_classification/no_tracking/partial_grids"
set VALIDN_TARGET_DIR_NAME="${TRAINING_TARGET_DIR_NAME}"

set BATCH_SIZES=(96 192 288 384 480 192 288 384 480 96 192 288 384 480 192 288 384 480 96 192 288 384 480 192 288 384 480 96 192 288 384 480 192 288 384 480 96 192 288 384 480 192 288 384 480 96 192 288 384 480 192 288 384 480 96 192 288 384 480 192 288 384 480 96 192 288 384 480 192 288 384 480 96 192 288 384 480 192 288 384 480)
set TRAINING_BATCH_COUNTS=(40 40 40 40 40 20 13 10 8 40 40 40 40 40 20 13 10 8 40 40 40 40 40 20 13 10 8 40 40 40 40 40 20 13 10 8 40 40 40 40 40 20 13 10 8 40 40 40 40 40 20 13 10 8 40 40 40 40 40 20 13 10 8 40 40 40 40 40 20 13 10 8 40 40 40 40 40 20 13 10 8)
set L2_WEIGHTS=(0.0000001000 0.0000001000 0.0000001000 0.0000001000 0.0000001000 0.0000001000 0.0000001000 0.0000001000 0.0000001000 0.0000003162 0.0000003162 0.0000003162 0.0000003162 0.0000003162 0.0000003162 0.0000003162 0.0000003162 0.0000003162 0.0000010000 0.0000010000 0.0000010000 0.0000010000 0.0000010000 0.0000010000 0.0000010000 0.0000010000 0.0000010000 0.0000031623 0.0000031623 0.0000031623 0.0000031623 0.0000031623 0.0000031623 0.0000031623 0.0000031623 0.0000031623 0.0000100000 0.0000100000 0.0000100000 0.0000100000 0.0000100000 0.0000100000 0.0000100000 0.0000100000 0.0000100000 0.0000316228 0.0000316228 0.0000316228 0.0000316228 0.0000316228 0.0000316228 0.0000316228 0.0000316228 0.0000316228 0.0001000000 0.0001000000 0.0001000000 0.0001000000 0.0001000000 0.0001000000 0.0001000000 0.0001000000 0.0001000000 0.0003162278 0.0003162278 0.0003162278 0.0003162278 0.0003162278 0.0003162278 0.0003162278 0.0003162278 0.0003162278 0.0010000000 0.0010000000 0.0010000000 0.0010000000 0.0010000000 0.0010000000 0.0010000000 0.0010000000 0.0010000000)

set batch_size=${BATCH_SIZES[$SLURM_ARRAY_TASK_ID]}
set training_batch_count=${TRAINING_BATCH_COUNTS[$SLURM_ARRAY_TASK_ID]}
set l2_weight=${L2_WEIGHTS[$SLURM_ARRAY_TASK_ID]}

set batch_size_string=`printf "%03d" $batch_size`
set training_batch_count_string=`printf "%02d" $training_batch_count`
set l2_weight_string=`printf "%.10f" $l2_weight`

set template_file_name="${TEMPLATE_DIR_NAME}/model_l2-weight=${l2_weight_string}.h5"
set output_dir_name="${TOP_OUTPUT_DIR_NAME}/batch-size=${batch_size_string}_training-batch-count=${training_batch_count_string}_l2-weight=${l2_weight_string}"
echo $output_dir_name

python3 -u "${CODE_DIR_NAME}/train_neural_net.py" \
--training_predictor_dir_name="${TRAINING_PREDICTOR_DIR_NAME}" \
--validn_predictor_dir_name="${VALIDN_PREDICTOR_DIR_NAME}" \
--training_target_dir_name="${TRAINING_TARGET_DIR_NAME}" \
--validn_target_dir_name="${VALIDN_TARGET_DIR_NAME}" \
--input_model_file_name="${template_file_name}" \
--output_model_dir_name="${output_dir_name}" \
--lead_time_seconds=0 \
--lag_times_seconds 0 600 \
--include_time_dimension=1 \
--normalize=1 \
--uniformize=1 \
--num_examples_per_batch=${batch_size} \
--max_examples_per_day_in_batch=`expr $batch_size / 8` \
--use_partial_grids=1 \
--num_epochs=1000 \
--num_training_batches_per_epoch=${training_batch_count} \
--num_validn_batches_per_epoch=`expr $training_batch_count / 8`
