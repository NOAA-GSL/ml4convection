#!/bin/tcsh

#SBATCH --job-name="apply_neural_nets"
#SBATCH --partition="fge"
#SBATCH --account="rda-ghpcs"
#SBATCH --qos="batch"
#SBATCH --ntasks=1
#SBATCH --nodes=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=32G
##SBATCH --nodes=1
##SBATCH --ntasks=1
##SBATCH --cpus-per-task=1
##SBATCH --ntasks-per-node=1
#SBATCH --time=12:00:00
#SBATCH --array=1-81
#SBATCH --exclude=h28n12
##SBATCH --exclude=h31n03,h28n12
##SBATCH --exclude=h29n01,h26n05,h28n06,h29n07,h30n03,h30n15
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=ryan.lagerquist@noaa.gov
#SBATCH --output=apply_neural_nets_%A_%a.out

module load cuda/10.1
source /scratch2/BMC/gsd-hpcs/Jebb.Q.Stewart/conda3.7/etc/profile.d/conda.csh
conda activate base

set CODE_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4convection_standalone/ml4convection"
set TOP_MODEL_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4convection_models/experiment13"
set TOP_PREDICTOR_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4convection_project/predictors/quality_controlled/partial_grids"
set TOP_TARGET_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4convection_project/targets/new_echo_classification/no_tracking/partial_grids"

set FIRST_VALIDN_DATE_STRING="20170101"
set LAST_VALIDN_DATE_STRING="20171224"

set BATCH_SIZES=(96 192 288 384 480 192 288 384 480 96 192 288 384 480 192 288 384 480 96 192 288 384 480 192 288 384 480 96 192 288 384 480 192 288 384 480 96 192 288 384 480 192 288 384 480 96 192 288 384 480 192 288 384 480 96 192 288 384 480 192 288 384 480 96 192 288 384 480 192 288 384 480 96 192 288 384 480 192 288 384 480)
set TRAINING_BATCH_COUNTS=(40 40 40 40 40 20 13 10 8 40 40 40 40 40 20 13 10 8 40 40 40 40 40 20 13 10 8 40 40 40 40 40 20 13 10 8 40 40 40 40 40 20 13 10 8 40 40 40 40 40 20 13 10 8 40 40 40 40 40 20 13 10 8 40 40 40 40 40 20 13 10 8 40 40 40 40 40 20 13 10 8)
set L2_WEIGHTS=(0.0000001000 0.0000001000 0.0000001000 0.0000001000 0.0000001000 0.0000001000 0.0000001000 0.0000001000 0.0000001000 0.0000003162 0.0000003162 0.0000003162 0.0000003162 0.0000003162 0.0000003162 0.0000003162 0.0000003162 0.0000003162 0.0000010000 0.0000010000 0.0000010000 0.0000010000 0.0000010000 0.0000010000 0.0000010000 0.0000010000 0.0000010000 0.0000031623 0.0000031623 0.0000031623 0.0000031623 0.0000031623 0.0000031623 0.0000031623 0.0000031623 0.0000031623 0.0000100000 0.0000100000 0.0000100000 0.0000100000 0.0000100000 0.0000100000 0.0000100000 0.0000100000 0.0000100000 0.0000316228 0.0000316228 0.0000316228 0.0000316228 0.0000316228 0.0000316228 0.0000316228 0.0000316228 0.0000316228 0.0001000000 0.0001000000 0.0001000000 0.0001000000 0.0001000000 0.0001000000 0.0001000000 0.0001000000 0.0001000000 0.0003162278 0.0003162278 0.0003162278 0.0003162278 0.0003162278 0.0003162278 0.0003162278 0.0003162278 0.0003162278 0.0010000000 0.0010000000 0.0010000000 0.0010000000 0.0010000000 0.0010000000 0.0010000000 0.0010000000 0.0010000000)

set batch_size=${BATCH_SIZES[$SLURM_ARRAY_TASK_ID]}
set training_batch_count=${TRAINING_BATCH_COUNTS[$SLURM_ARRAY_TASK_ID]}
set l2_weight=${L2_WEIGHTS[$SLURM_ARRAY_TASK_ID]}

set batch_size_string=`printf "%03d" $batch_size`
set training_batch_count_string=`printf "%02d" $training_batch_count`
set l2_weight_string=`printf "%.10f" $l2_weight`

set model_dir_name="${TOP_MODEL_DIR_NAME}/batch-size=${batch_size_string}_training-batch-count=${training_batch_count_string}_l2-weight=${l2_weight_string}"
set model_file_name="${model_dir_name}/model.h5"
set prediction_dir_name="${model_dir_name}/validation/partial_grids"

python3 -u "${CODE_DIR_NAME}/apply_neural_net.py" \
--input_model_file_name="${model_file_name}" \
--input_predictor_dir_name="${TOP_PREDICTOR_DIR_NAME}" \
--input_target_dir_name="${TOP_TARGET_DIR_NAME}" \
--apply_to_full_grids=0 \
--first_valid_date_string="${FIRST_VALIDN_DATE_STRING}" \
--last_valid_date_string="${LAST_VALIDN_DATE_STRING}" \
--output_dir_name="${prediction_dir_name}"
